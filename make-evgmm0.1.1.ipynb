{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEVCは声質変換の一つの手法です.\\n詳細 : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580abf534c4dbb8bc.pdf\\n\\nEVC is kind of Voice Conversion(VC).\\nCheck detail : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580abf534c4dbb8bc.pdf\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "EVCのためのEV-GMMを構築します. そして, 適応学習する.\n",
    "詳細 : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580aabf534c4dbb8bc.pdf\n",
    "\n",
    "This program make EV-GMM for EVC. Then, it make adaptation learning.\n",
    "Check detail : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580abf534c4dbb8bc.pdf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "from shutil import rmtree\n",
    "import argparse\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GMM # sklearn 0.20.0から使えない\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.signal\n",
    "import scipy.sparse\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython \n",
    "from IPython.display import Audio \n",
    "\n",
    "import soundfile as sf\n",
    "import wave \n",
    "import pyworld as pw\n",
    "import librosa.display\n",
    "\n",
    "from dtw import dtw\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\n",
    "__Mixtured : GMM混合数\n",
    "__versions : 実験セット\n",
    "__convert_source : 変換元話者のパス\n",
    "__convert_target : 変換先話者のパス\n",
    "\"\"\"\n",
    "# parameters \n",
    "__Mixtured = 40\n",
    "__versions = 'pre-stored0.1.1'\n",
    "__convert_source = 'input/EJM10/V01/T01/TIMIT/000/*.wav' \n",
    "__convert_target = 'adaptation/EJM04/V01/T01/ATR503/A/*.wav'\n",
    "\n",
    "# settings\n",
    "__same_path = './utterance/' + __versions + '/'\n",
    "\n",
    "Mixtured = __Mixtured\n",
    "pre_stored_pickle = __same_path + __versions + '.pickle'\n",
    "pre_stored_source_list = __same_path + 'pre-source/**/V01/T01/**/*.wav'\n",
    "pre_stored_list = __same_path + \"pre/**/V01/T01/**/*.wav\"\n",
    "#pre_stored_target_list = \"\" (not yet)\n",
    "pre_stored_gmm_init_pickle = __same_path + __versions + '_init-gmm.pickle'\n",
    "pre_stored_sv_npy = __same_path + __versions + '_sv.npy'\n",
    "\n",
    "save_for_evgmm_covarXX = __same_path + __versions + '_covarXX.npy'\n",
    "save_for_evgmm_covarYX = __same_path + __versions + '_covarYX.npy'\n",
    "save_for_evgmm_fitted_source = __same_path + __versions + '_fitted_source.npy'\n",
    "save_for_evgmm_fitted_target = __same_path + __versions + '_fitted_target.npy'\n",
    "save_for_evgmm_weights = __same_path + __versions + '_weights.npy'\n",
    "save_for_evgmm_source_means = __same_path + __versions + '_source_means.npy'\n",
    "\n",
    "for_convert_source = __same_path + __convert_source\n",
    "for_convert_target = __same_path + __convert_target\n",
    "converted_voice_npy = __same_path + 'sp_converted_' + __versions  \n",
    "converted_voice_wav = __same_path + 'sp_converted_' + __versions \n",
    "mfcc_save_fig_png = __same_path + 'mfcc3dim_' + __versions \n",
    "f0_save_fig_png = __same_path + 'f0_converted' + __versions\n",
    "converted_voice_with_f0_wav = __same_path + 'sp_f0_converted' + __versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "class MFCC:\n",
    "    \"\"\"\n",
    "    MFCC() : メル周波数ケプストラム係数(MFCC)を求めたり、MFCCからスペクトルに変換したりするクラス.\n",
    "    動的特徴量(delta)が実装途中.\n",
    "    ref : http://aidiary.hatenablog.com/entry/20120225/1330179868\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, frequency, nfft=1026, dimension=24, channels=24):\n",
    "        \"\"\"\n",
    "        各種パラメータのセット\n",
    "        nfft : FFTのサンプル点数\n",
    "        frequency : サンプリング周波数\n",
    "        dimension : MFCC次元数\n",
    "        channles : メルフィルタバンクのチャンネル数(dimensionに依存)\n",
    "        fscale : 周波数スケール軸\n",
    "        filterbankl, fcenters : フィルタバンク行列, フィルタバンクの頂点(?)\n",
    "        \"\"\"\n",
    "        self.nfft = nfft\n",
    "        self.frequency = frequency\n",
    "        self.dimension = dimension\n",
    "        self.channels = channels\n",
    "        self.fscale = np.fft.fftfreq(self.nfft, d = 1.0 / self.frequency)[: int(self.nfft / 2)]\n",
    "        self.filterbank, self.fcenters = self.melFilterBank()\n",
    "    \n",
    "    def hz2mel(self, f):\n",
    "        \"\"\"\n",
    "        周波数からメル周波数に変換\n",
    "        \"\"\"\n",
    "        return 1127.01048 * np.log(f / 700.0 + 1.0)\n",
    "    \n",
    "    def mel2hz(self, m):\n",
    "        \"\"\"\n",
    "        メル周波数から周波数に変換\n",
    "        \"\"\"     \n",
    "        return 700.0 * (np.exp(m / 1127.01048) - 1.0)\n",
    "\n",
    "    def melFilterBank(self):\n",
    "        \"\"\"\n",
    "        メルフィルタバンクを生成する\n",
    "        \"\"\"      \n",
    "        fmax = self.frequency / 2\n",
    "        melmax = self.hz2mel(fmax)\n",
    "        nmax = int(self.nfft / 2)\n",
    "        df = self.frequency / self.nfft\n",
    "        dmel = melmax / (self.channels + 1)\n",
    "        melcenters = np.arange(1, self.channels + 1) * dmel\n",
    "        fcenters = self.mel2hz(melcenters)\n",
    "        indexcenter = np.round(fcenters / df)\n",
    "        indexstart = np.hstack(([0], indexcenter[0:self.channels - 1]))\n",
    "        indexstop = np.hstack((indexcenter[1:self.channels], [nmax]))\n",
    "\n",
    "        filterbank = np.zeros((self.channels, nmax))\n",
    "        for c in np.arange(0, self.channels):\n",
    "            increment = 1.0 / (indexcenter[c] - indexstart[c])\n",
    "            # np,int_ は np.arangeが[0. 1. 2. ..]となるのをintにする\n",
    "            for i in np.int_(np.arange(indexstart[c], indexcenter[c])):\n",
    "                filterbank[c, i] = (i - indexstart[c]) * increment\n",
    "            decrement = 1.0 / (indexstop[c] - indexcenter[c])\n",
    "            # np,int_ は np.arangeが[0. 1. 2. ..]となるのをintにする\n",
    "            for i in np.int_(np.arange(indexcenter[c], indexstop[c])):\n",
    "                filterbank[c, i] = 1.0 - ((i - indexcenter[c]) * decrement)\n",
    "\n",
    "        return filterbank, fcenters\n",
    "    \n",
    "    def mfcc(self, spectrum):\n",
    "        \"\"\"\n",
    "        スペクトルからMFCCを求める.\n",
    "        \"\"\"\n",
    "        mspec = []\n",
    "        mspec = np.log10(np.dot(spectrum, self.filterbank.T))\n",
    "        mspec = np.array(mspec)\n",
    "        \n",
    "        return scipy.fftpack.realtransforms.dct(mspec, type=2, norm=\"ortho\", axis=-1)\n",
    "    \n",
    "    def delta(self, mfcc):\n",
    "        \"\"\"\n",
    "        MFCCから動的特徴量を求める.\n",
    "        現在は,求める特徴量フレームtをt-1とt+1の平均としている.\n",
    "        \"\"\"\n",
    "        mfcc = np.concatenate([\n",
    "            [mfcc[0]], \n",
    "            mfcc, \n",
    "            [mfcc[-1]]\n",
    "        ]) # 最初のフレームを最初に、最後のフレームを最後に付け足す\n",
    "        delta = None\n",
    "        for i in range(1, mfcc.shape[0] - 1):\n",
    "            slope = (mfcc[i+1] - mfcc[i-1]) / 2\n",
    "            if delta is None:\n",
    "                delta = slope\n",
    "            else:\n",
    "                delta = np.vstack([delta, slope])\n",
    "        return delta\n",
    "    \n",
    "    def imfcc(self, mfcc, spectrogram):\n",
    "        \"\"\"\n",
    "        MFCCからスペクトルを求める.\n",
    "        \"\"\"\n",
    "        im_sp = np.array([])\n",
    "        for i in range(mfcc.shape[0]):\n",
    "            mfcc_s = np.hstack([mfcc[i], [0] * (self.channels - self.dimension)])\n",
    "            mspectrum = scipy.fftpack.idct(mfcc_s, norm='ortho')\n",
    "            # splrep はスプライン補間のための補間関数を求める\n",
    "            tck = scipy.interpolate.splrep(self.fcenters, np.power(10, mspectrum))\n",
    "            # splev は指定座標での補間値を求める\n",
    "            im_spectrogram = scipy.interpolate.splev(self.fscale, tck)\n",
    "            im_sp = np.concatenate((im_sp, im_spectrogram), axis=0)\n",
    "            \n",
    "        return im_sp.reshape(spectrogram.shape)\n",
    "            \n",
    "    def trim_zeros_frames(x, eps=1e-7):\n",
    "        \"\"\"\n",
    "        無音区間を取り除く.\n",
    "        \"\"\"\n",
    "        T, D = x.shape\n",
    "        s = np.sum(np.abs(x), axis=1)\n",
    "        s[s < 1e-7] = 0.\n",
    "        return x[s > eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_by_world_with_harverst(x, fs):\n",
    "    \"\"\"\n",
    "    WORLD音声分析合成器で基本周波数F0,スペクトル包絡,非周期成分を求める.\n",
    "    基本周波数F0についてはharvest法により,より精度良く求める.\n",
    "    \"\"\"\n",
    "    # 4 Harvest with F0 refinement (using Stonemask)\n",
    "    frame_period = 5\n",
    "    _f0_h, t_h = pw.harvest(x, fs, frame_period=frame_period)\n",
    "    f0_h = pw.stonemask(x, _f0_h, t_h, fs)\n",
    "    sp_h = pw.cheaptrick(x, f0_h, t_h, fs)\n",
    "    ap_h = pw.d4c(x, f0_h, t_h, fs)\n",
    "    \n",
    "    return f0_h, sp_h, ap_h\n",
    "\n",
    "def wavread(file):\n",
    "    \"\"\"\n",
    "    wavファイルから音声トラックとサンプリング周波数を抽出する.\n",
    "    \"\"\"\n",
    "    wf = wave.open(file, \"r\")\n",
    "    fs = wf.getframerate()\n",
    "    x = wf.readframes(wf.getnframes())\n",
    "    x = np.frombuffer(x, dtype= \"int16\") / 32768.0\n",
    "    wf.close()\n",
    "    return x, float(fs)\n",
    "\n",
    "def preEmphasis(signal, p=0.97):\n",
    "    \"\"\"\n",
    "    MFCC抽出のための高域強調フィルタ.\n",
    "    波形を通すことで,高域成分が強調される.\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1.0, -p], 1, signal)\n",
    "\n",
    "def alignment(source, target, path):\n",
    "    \"\"\"\n",
    "    タイムアライメントを取る.\n",
    "    target音声をsource音声の長さに合うように調整する.\n",
    "    \"\"\"\n",
    "    # ここでは814に合わせよう(targetに合わせる)\n",
    "    # p_p = 0 if source.shape[0] > target.shape[0] else 1\n",
    "\n",
    "    #shapes = source.shape if source.shape[0] > target.shape[0] else target.shape \n",
    "    shapes = source.shape\n",
    "    align = np.array([])\n",
    "    for (i, p) in enumerate(path[0]):\n",
    "        if i != 0:\n",
    "            if j != p:\n",
    "                temp = np.array(target[path[1][i]])\n",
    "                align = np.concatenate((align, temp), axis=0)\n",
    "        else:\n",
    "            temp = np.array(target[path[1][i]])\n",
    "            align = np.concatenate((align, temp), axis=0)   \n",
    "        \n",
    "        j = p\n",
    "        \n",
    "    return align.reshape(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist,  ./utterance/pre-stored0.1.1/pre-stored0.1.1.pickle\n",
      "open,  ./utterance/pre-stored0.1.1/pre-stored0.1.1.pickle\n",
      "Load pre-stored time =  2.1454219818115234 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "pre-stored学習のためのパラレル学習データを作る。\n",
    "時間がかかるため、利用できるlearn-data.pickleがある場合はそれを利用する。\n",
    "それがない場合は一から作り直す。\n",
    "\"\"\"\n",
    "timer_start = time.time()\n",
    "if os.path.exists(pre_stored_pickle):\n",
    "    print(\"exist, \", pre_stored_pickle)\n",
    "    with open(pre_stored_pickle, mode='rb') as f:\n",
    "        total_data = pickle.load(f)\n",
    "        print(\"open, \", pre_stored_pickle)\n",
    "        print(\"Load pre-stored time = \", time.time() - timer_start , \"[sec]\")\n",
    "else:\n",
    "    source_mfcc = []\n",
    "    #source_data_sets = []\n",
    "    for name in sorted(glob.iglob(pre_stored_source_list, recursive=True)):\n",
    "        print(name)\n",
    "        x, fs = sf.read(name)\n",
    "        f0, sp, ap = analyse_by_world_with_harverst(x, fs)\n",
    "    \n",
    "        mfcc = MFCC(fs)\n",
    "        source_mfcc_temp = mfcc.mfcc(sp)\n",
    "        #source_data = np.hstack([source_mfcc_temp, mfcc.delta(source_mfcc_temp)]) # static & dynamic featuers\n",
    "        source_mfcc.append(source_mfcc_temp)\n",
    "        #source_data_sets.append(source_data)\n",
    "\n",
    "    total_data = []\n",
    "    \n",
    "    i = 0\n",
    "    _s_len = len(source_mfcc)\n",
    "    for name in sorted(glob.iglob(pre_stored_list, recursive=True)):\n",
    "        print(name, len(total_data))\n",
    "        x, fs = sf.read(name)\n",
    "        f0, sp, ap = analyse_by_world_with_harverst(x, fs)\n",
    "\n",
    "        mfcc = MFCC(fs)\n",
    "        target_mfcc = mfcc.mfcc(sp)\n",
    "\n",
    "        dist, cost, acc, path = dtw(source_mfcc[i%_s_len], target_mfcc, dist=lambda x, y: norm(x - y, ord=1))\n",
    "        #print('Normalized distance between the two sounds:' + str(dist))\n",
    "        #print(\"target_mfcc = {0}\".format(target_mfcc.shape))\n",
    "\n",
    "        aligned = alignment(source_mfcc[i%_s_len], target_mfcc, path)\n",
    "        #target_data_sets = np.hstack([aligned, mfcc.delta(aligned)]) # static & dynamic features\n",
    "        #learn_data = np.hstack((source_data_sets[i], target_data_sets))\n",
    "        learn_data = np.hstack([source_mfcc[i%_s_len], aligned])\n",
    "\n",
    "        total_data.append(learn_data)\n",
    "        i += 1\n",
    "        \n",
    "    with open(pre_stored_pickle, 'wb') as output:\n",
    "        pickle.dump(total_data, output)\n",
    "        print(\"Make, \", pre_stored_pickle)\n",
    "        print(\"Make pre-stored time = \", time.time() - timer_start , \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_data[0].shape =  (959, 48)\n",
      "S =  1012\n",
      "D =  24\n",
      "exist,  ./utterance/pre-stored0.1.1/pre-stored0.1.1_init-gmm.pickle\n",
      "open,  ./utterance/pre-stored0.1.1/pre-stored0.1.1_init-gmm.pickle\n",
      "Load initial_gmm time =  0.19963502883911133 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "全事前学習出力話者からラムダを推定する.\n",
    "ラムダは適応学習で変容する.\n",
    "\"\"\"\n",
    "\n",
    "S = len(total_data)\n",
    "D = int(total_data[0].shape[1] / 2)\n",
    "print(\"total_data[0].shape = \", total_data[0].shape)\n",
    "print(\"S = \", S)\n",
    "print(\"D = \", D)\n",
    "\n",
    "timer_start = time.time()\n",
    "if os.path.exists(pre_stored_gmm_init_pickle):\n",
    "    print(\"exist, \", pre_stored_gmm_init_pickle)\n",
    "    with open(pre_stored_gmm_init_pickle, mode='rb') as f:\n",
    "        initial_gmm = pickle.load(f)\n",
    "        print(\"open, \", pre_stored_gmm_init_pickle)\n",
    "        print(\"Load initial_gmm time = \", time.time() - timer_start , \"[sec]\")  \n",
    "else:\n",
    "    initial_gmm = GMM(n_components = Mixtured, covariance_type = 'full')\n",
    "    initial_gmm.fit(np.vstack(total_data))\n",
    "    with open(pre_stored_gmm_init_pickle, 'wb') as output:\n",
    "        pickle.dump(initial_gmm, output)\n",
    "        print(\"Make, \", initial_gmm)\n",
    "        print(\"Make initial_gmm time = \", time.time() - timer_start , \"[sec]\")    \n",
    "            \n",
    "weights = initial_gmm.weights_\n",
    "source_means = initial_gmm.means_[:, :D]\n",
    "target_means = initial_gmm.means_[:, D:]\n",
    "covarXX = initial_gmm.covars_[:, :D, :D]\n",
    "covarXY = initial_gmm.covars_[:, :D, D:]\n",
    "covarYX = initial_gmm.covars_[:, D:, :D]\n",
    "covarYY = initial_gmm.covars_[:, D:, D:]\n",
    "\n",
    "fitted_source = source_means\n",
    "fitted_target = target_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist,  ./utterance/pre-stored0.1.1/pre-stored0.1.1_sv.npy\n",
      "open,  ./utterance/pre-stored0.1.1/pre-stored0.1.1_sv.npy\n",
      "Load pre_stored_sv time =  0.3458998203277588 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SVはGMMスーパーベクトルで、各pre-stored学習における出力話者について平均ベクトルを推定する。\n",
    "GMMの学習を見てみる必要があるか？\n",
    "\"\"\"\n",
    "\n",
    "timer_start = time.time()\n",
    "if os.path.exists(pre_stored_sv_npy):\n",
    "    print(\"exist, \", pre_stored_sv_npy)\n",
    "    sv = np.load(pre_stored_sv_npy)\n",
    "    print(\"open, \", pre_stored_sv_npy)\n",
    "    print(\"Load pre_stored_sv time = \", time.time() - timer_start , \"[sec]\")  \n",
    "    \n",
    "else:\n",
    "    sv = []\n",
    "    for i in range(S):\n",
    "        gmm = GMM(n_components = Mixtured, params = 'm', init_params = '', covariance_type = 'full')\n",
    "        gmm.weights_ = initial_gmm.weights_\n",
    "        gmm.means_ = initial_gmm.means_\n",
    "        gmm.covars_ = initial_gmm.covars_\n",
    "        gmm.fit(total_data[i])\n",
    "        sv.append(gmm.means_)\n",
    "    sv = np.array(sv)\n",
    "    np.save(pre_stored_sv_npy, sv)\n",
    "    print(\"Make pre_stored_sv time = \", time.time() - timer_start , \"[sec]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960,)\n",
      "(960, 960)\n",
      "(1012, 960)\n",
      "(960,)\n",
      "(960, 960)\n",
      "(1012, 960)\n",
      "Do PCA time =  5.670940160751343 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "各事前学習出力話者のGMM平均ベクトルに対して主成分分析(PCA)を行う.\n",
    "PCAで求めた固有値と固有ベクトルからeigenvectorsとbiasvectorsを作る.\n",
    "\"\"\"\n",
    "timer_start = time.time()\n",
    "#source_pca\n",
    "source_n_component, source_n_features = sv[:, :, :D].reshape(S, Mixtured*D).shape\n",
    "# 標準化(分散を1、平均を0にする)\n",
    "source_stdsc = StandardScaler()\n",
    "# 共分散行列を求める\n",
    "source_X_std = source_stdsc.fit_transform(sv[:, :, :D].reshape(S, Mixtured*D)) \n",
    "\n",
    "# PCAを行う\n",
    "source_cov = source_X_std.T @ source_X_std / (source_n_component - 1)\n",
    "source_W, source_V_pca = np.linalg.eig(source_cov)\n",
    "\n",
    "print(source_W.shape)\n",
    "print(source_V_pca.shape)\n",
    "\n",
    "# データを主成分の空間に変換する\n",
    "source_X_pca = source_X_std @ source_V_pca\n",
    "print(source_X_pca.shape)\n",
    "\n",
    "#target_pca\n",
    "target_n_component, target_n_features = sv[:, :, D:].reshape(S, Mixtured*D).shape\n",
    "# 標準化(分散を1、平均を0にする)\n",
    "target_stdsc = StandardScaler()\n",
    "#共分散行列を求める\n",
    "target_X_std = target_stdsc.fit_transform(sv[:, :, D:].reshape(S, Mixtured*D)) \n",
    "\n",
    "#PCAを行う\n",
    "target_cov = target_X_std.T @ target_X_std / (target_n_component - 1)\n",
    "target_W, target_V_pca = np.linalg.eig(target_cov)\n",
    "\n",
    "print(target_W.shape)\n",
    "print(target_V_pca.shape)\n",
    "\n",
    "# データを主成分の空間に変換する\n",
    "target_X_pca = target_X_std @ target_V_pca\n",
    "print(target_X_pca.shape)\n",
    "\n",
    "eigenvectors = source_X_pca.reshape((Mixtured, D, S)), target_X_pca.reshape((Mixtured, D, S))\n",
    "source_bias = np.mean(sv[:, :, :D], axis=0)\n",
    "target_bias = np.mean(sv[:, :, D:], axis=0)\n",
    "biasvectors = source_bias.reshape((Mixtured, D)), target_bias.reshape((Mixtured, D))\n",
    "\n",
    "print(\"Do PCA time = \", time.time() - timer_start , \"[sec]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A11.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A14.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A17.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A18.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A19.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A20.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A21.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A22.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A23.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A24.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A25.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A26.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A27.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A28.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A29.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A30.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A31.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A32.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A33.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A34.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A35.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A36.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A37.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A38.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A39.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A40.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A41.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A42.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A43.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A44.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A45.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A46.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A47.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A48.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A49.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A50.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A51.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A52.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A53.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A54.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A55.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A56.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A57.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A58.wav\n",
      "source =  ./utterance/pre-stored0.1.1/input/EJM10/V01/T01/TIMIT/000/A59.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A01.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A02.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A03.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A05.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A06.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A07.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A08.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A09.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A10.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A11.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A13.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A14.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A15.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A16.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A17.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A18.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A19.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A20.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A21.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A22.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A23.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A24.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A25.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A26.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A27.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A28.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A29.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A30.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A31.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A32.wav\n",
      "target =  ./utterance/pre-stored0.1.1/adaptation/EJM04/V01/T01/ATR503/A/A33.wav\n",
      "Load Input and Target Voice time =  132.16769576072693 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "声質変換に用いる変換元音声と目標音声を読み込む.\n",
    "\"\"\"\n",
    "\n",
    "timer_start = time.time()\n",
    "source_mfcc_for_convert = []\n",
    "source_sp_for_convert = []\n",
    "source_f0_for_convert = []\n",
    "source_ap_for_convert = []\n",
    "fs_source = None\n",
    "for name in sorted(glob.iglob(for_convert_source, recursive=True)):\n",
    "    print(\"source = \", name)\n",
    "    x_source, fs_source = sf.read(name)\n",
    "    f0_source, sp_source, ap_source = analyse_by_world_with_harverst(x_source, fs_source)\n",
    "    mfcc_source = MFCC(fs_source)\n",
    "    #mfcc_s_tmp = mfcc_s.mfcc(sp)\n",
    "    #source_mfcc_for_convert = np.hstack([mfcc_s_tmp, mfcc_s.delta(mfcc_s_tmp)])\n",
    "    source_mfcc_for_convert.append(mfcc_source.mfcc(sp_source))\n",
    "    source_sp_for_convert.append(sp_source)\n",
    "    source_f0_for_convert.append(f0_source)\n",
    "    source_ap_for_convert.append(ap_source)\n",
    "\n",
    "target_mfcc_for_fit = []\n",
    "target_f0_for_fit = []\n",
    "target_ap_for_fit = []\n",
    "for name in sorted(glob.iglob(for_convert_target, recursive=True)):\n",
    "    print(\"target = \", name)\n",
    "    x_target, fs_target = sf.read(name)\n",
    "    f0_target, sp_target, ap_target = analyse_by_world_with_harverst(x_target, fs_target)\n",
    "    mfcc_target = MFCC(fs_target)\n",
    "    #mfcc_target_tmp = mfcc_target.mfcc(sp_target)\n",
    "    #target_mfcc_for_fit = np.hstack([mfcc_t_tmp, mfcc_t.delta(mfcc_t_tmp)])\n",
    "    target_mfcc_for_fit.append(mfcc_target.mfcc(sp_target))\n",
    "    target_f0_for_fit.append(f0_target)\n",
    "    target_ap_for_fit.append(ap_target)\n",
    "\n",
    "# 全部numpy.arrrayにしておく\n",
    "source_data_mfcc = np.array(source_mfcc_for_convert)\n",
    "source_data_sp = np.array(source_sp_for_convert)\n",
    "source_data_f0 = np.array(source_f0_for_convert)\n",
    "source_data_ap = np.array(source_ap_for_convert)\n",
    "\n",
    "target_mfcc = np.array(target_mfcc_for_fit)\n",
    "target_f0 = np.array(target_f0_for_fit)\n",
    "target_ap = np.array(target_ap_for_fit)\n",
    "\n",
    "print(\"Load Input and Target Voice time = \", time.time() - timer_start , \"[sec]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db8c48b506c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtimer_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "適応話者学習を行う.\n",
    "つまり,事前学習出力話者から目標話者の空間を作りだす.\n",
    "\n",
    "適応話者文数ごとにfitted_targetを集めるのは未実装.\n",
    "\"\"\"\n",
    "\n",
    "timer_start = time.time()\n",
    "epoch=1000\n",
    "\n",
    "py = GMM(n_components = Mixtured, covariance_type = 'full')\n",
    "py.weights_ = weights\n",
    "py.means_ = target_means\n",
    "py.covars_ = covarYY\n",
    "\n",
    "fitted_target = None\n",
    "\n",
    "for i in range(len(target_mfcc)):\n",
    "    print(\"adaptation = \", i+1, \"/\", len(target_mfcc))\n",
    "    target = target_mfcc[i]\n",
    "\n",
    "    for x in range(epoch):\n",
    "        print(\"epoch = \", x)\n",
    "        predict = py.predict_proba(np.atleast_2d(target))\n",
    "        y = np.sum([predict[:, i: i + 1] * (target - biasvectors[1][i])\n",
    "                    for i in range(Mixtured)], axis = 1)\n",
    "        gamma = np.sum(predict, axis = 0)\n",
    "\n",
    "        left = np.sum([gamma[i] * np.dot(eigenvectors[1][i].T,\n",
    "                                         np.linalg.solve(py.covars_, eigenvectors[1])[i])\n",
    "                       for i in range(Mixtured)], axis=0)\n",
    "        right = np.sum([np.dot(eigenvectors[1][i].T, \n",
    "                               np.linalg.solve(py.covars_, y)[i]) \n",
    "                        for i in range(Mixtured)], axis = 0)\n",
    "        weight = np.linalg.solve(left, right)\n",
    "\n",
    "        fitted_target = np.dot(eigenvectors[1], weight) + biasvectors[1]\n",
    "        py.means_ = fitted_target\n",
    "        \n",
    "print(\"Load Input and Target Voice time = \", time.time() - timer_start , \"[sec]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "変換に必要なものを残しておく.\n",
    "\"\"\"\n",
    "np.save(save_for_evgmm_covarXX, covarXX)\n",
    "np.save(save_for_evgmm_covarYX, covarYX)\n",
    "np.save(save_for_evgmm_fitted_source, fitted_source)\n",
    "np.save(save_for_evgmm_fitted_target, fitted_target)\n",
    "np.save(save_for_evgmm_weights, weights)\n",
    "np.save(save_for_evgmm_source_means, source_means)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
