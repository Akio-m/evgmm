{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEVCで変換する.\\n詳細 : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580aabf534c4dbb8bc.pdf\\n\\nConverting by EVC.\\nCheck detail : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580abf534c4dbb8bc.pdf\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "EVCで変換する.\n",
    "詳細 : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580aabf534c4dbb8bc.pdf\n",
    "\n",
    "Converting by EVC.\n",
    "Check detail : https://pdfs.semanticscholar.org/cbfe/71798ded05fb8bf8674580abf534c4dbb8bc.pdf\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os\n",
    "from shutil import rmtree\n",
    "import argparse\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GMM # sklearn 0.20.0から使えない\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.signal\n",
    "import scipy.sparse\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython \n",
    "from IPython.display import Audio \n",
    "\n",
    "import soundfile as sf\n",
    "import wave \n",
    "import pyworld as pw\n",
    "import librosa.display\n",
    "\n",
    "from dtw import dtw\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\n",
    "__Mixtured : GMM混合数\n",
    "__versions : 実験セット\n",
    "__convert_source : 変換元話者のパス\n",
    "__convert_target : 変換先話者のパス\n",
    "\"\"\"\n",
    "# parameters \n",
    "__Mixtured = 40\n",
    "__versions = 'pre-stored0.1.3'\n",
    "__convert_source = 'input/EJM10/V01/T01/TIMIT/000/*.wav' \n",
    "__convert_target = 'adaptation/EJF01/V01/T01/ATR503/A/*.wav'\n",
    "\n",
    "# settings\n",
    "__same_path = './utterance/' + __versions + '/'\n",
    "__output_path = __same_path + 'output/EJF01/' # EJF01, EJF07, EJM04, EJM05\n",
    "\n",
    "Mixtured = __Mixtured\n",
    "pre_stored_pickle = __same_path + __versions + '.pickle'\n",
    "pre_stored_source_list = __same_path + 'pre-source/**/V01/T01/**/*.wav'\n",
    "pre_stored_list = __same_path + \"pre/**/V01/T01/**/*.wav\"\n",
    "#pre_stored_target_list = \"\" (not yet)\n",
    "pre_stored_gmm_init_pickle = __same_path + __versions + '_init-gmm.pickle'\n",
    "pre_stored_sv_npy = __same_path + __versions + '_sv.npy'\n",
    "\n",
    "save_for_evgmm_covarXX = __output_path + __versions + '_covarXX.npy'\n",
    "save_for_evgmm_covarYX = __output_path + __versions + '_covarYX.npy'\n",
    "save_for_evgmm_fitted_source = __output_path + __versions + '_fitted_source.npy'\n",
    "save_for_evgmm_fitted_target = __output_path + __versions + '_fitted_target.npy'\n",
    "save_for_evgmm_weights = __output_path + __versions + '_weights.npy'\n",
    "save_for_evgmm_source_means = __output_path + __versions + '_source_means.npy'\n",
    "\n",
    "for_convert_source = __same_path + __convert_source\n",
    "for_convert_target = __same_path + __convert_target\n",
    "converted_voice_npy = __output_path + 'sp_converted_' + __versions  \n",
    "converted_voice_wav = __output_path + 'sp_converted_' + __versions \n",
    "mfcc_save_fig_png = __output_path + 'mfcc3dim_' + __versions \n",
    "f0_save_fig_png = __output_path + 'f0_converted' + __versions\n",
    "converted_voice_with_f0_wav = __output_path + 'sp_f0_converted' + __versions\n",
    "\n",
    "__measure_target = 'adaptation/EJF01/V01/T01/TIMIT/000/*.wav'\n",
    "for_measure_target = __same_path + __measure_target\n",
    "mcd_text = __output_path + __versions + '_MCD.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "class MFCC:\n",
    "    \"\"\"\n",
    "    MFCC() : メル周波数ケプストラム係数(MFCC)を求めたり、MFCCからスペクトルに変換したりするクラス.\n",
    "    動的特徴量(delta)が実装途中.\n",
    "    ref : http://aidiary.hatenablog.com/entry/20120225/1330179868\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, frequency, nfft=1026, dimension=24, channels=24):\n",
    "        \"\"\"\n",
    "        各種パラメータのセット\n",
    "        nfft : FFTのサンプル点数\n",
    "        frequency : サンプリング周波数\n",
    "        dimension : MFCC次元数\n",
    "        channles : メルフィルタバンクのチャンネル数(dimensionに依存)\n",
    "        fscale : 周波数スケール軸\n",
    "        filterbankl, fcenters : フィルタバンク行列, フィルタバンクの頂点(?)\n",
    "        \"\"\"\n",
    "        self.nfft = nfft\n",
    "        self.frequency = frequency\n",
    "        self.dimension = dimension\n",
    "        self.channels = channels\n",
    "        self.fscale = np.fft.fftfreq(self.nfft, d = 1.0 / self.frequency)[: int(self.nfft / 2)]\n",
    "        self.filterbank, self.fcenters = self.melFilterBank()\n",
    "    \n",
    "    def hz2mel(self, f):\n",
    "        \"\"\"\n",
    "        周波数からメル周波数に変換\n",
    "        \"\"\"\n",
    "        return 1127.01048 * np.log(f / 700.0 + 1.0)\n",
    "    \n",
    "    def mel2hz(self, m):\n",
    "        \"\"\"\n",
    "        メル周波数から周波数に変換\n",
    "        \"\"\"     \n",
    "        return 700.0 * (np.exp(m / 1127.01048) - 1.0)\n",
    "\n",
    "    def melFilterBank(self):\n",
    "        \"\"\"\n",
    "        メルフィルタバンクを生成する\n",
    "        \"\"\"      \n",
    "        fmax = self.frequency / 2\n",
    "        melmax = self.hz2mel(fmax)\n",
    "        nmax = int(self.nfft / 2)\n",
    "        df = self.frequency / self.nfft\n",
    "        dmel = melmax / (self.channels + 1)\n",
    "        melcenters = np.arange(1, self.channels + 1) * dmel\n",
    "        fcenters = self.mel2hz(melcenters)\n",
    "        indexcenter = np.round(fcenters / df)\n",
    "        indexstart = np.hstack(([0], indexcenter[0:self.channels - 1]))\n",
    "        indexstop = np.hstack((indexcenter[1:self.channels], [nmax]))\n",
    "\n",
    "        filterbank = np.zeros((self.channels, nmax))\n",
    "        for c in np.arange(0, self.channels):\n",
    "            increment = 1.0 / (indexcenter[c] - indexstart[c])\n",
    "            # np,int_ は np.arangeが[0. 1. 2. ..]となるのをintにする\n",
    "            for i in np.int_(np.arange(indexstart[c], indexcenter[c])):\n",
    "                filterbank[c, i] = (i - indexstart[c]) * increment\n",
    "            decrement = 1.0 / (indexstop[c] - indexcenter[c])\n",
    "            # np,int_ は np.arangeが[0. 1. 2. ..]となるのをintにする\n",
    "            for i in np.int_(np.arange(indexcenter[c], indexstop[c])):\n",
    "                filterbank[c, i] = 1.0 - ((i - indexcenter[c]) * decrement)\n",
    "\n",
    "        return filterbank, fcenters\n",
    "    \n",
    "    def mfcc(self, spectrum):\n",
    "        \"\"\"\n",
    "        スペクトルからMFCCを求める.\n",
    "        \"\"\"\n",
    "        mspec = []\n",
    "        mspec = np.log10(np.dot(spectrum, self.filterbank.T))\n",
    "        mspec = np.array(mspec)\n",
    "        \n",
    "        return scipy.fftpack.realtransforms.dct(mspec, type=2, norm=\"ortho\", axis=-1)\n",
    "    \n",
    "    def delta(self, mfcc):\n",
    "        \"\"\"\n",
    "        MFCCから動的特徴量を求める.\n",
    "        現在は,求める特徴量フレームtをt-1とt+1の平均としている.\n",
    "        \"\"\"\n",
    "        mfcc = np.concatenate([\n",
    "            [mfcc[0]], \n",
    "            mfcc, \n",
    "            [mfcc[-1]]\n",
    "        ]) # 最初のフレームを最初に、最後のフレームを最後に付け足す\n",
    "        delta = None\n",
    "        for i in range(1, mfcc.shape[0] - 1):\n",
    "            slope = (mfcc[i+1] - mfcc[i-1]) / 2\n",
    "            if delta is None:\n",
    "                delta = slope\n",
    "            else:\n",
    "                delta = np.vstack([delta, slope])\n",
    "        return delta\n",
    "    \n",
    "    def imfcc(self, mfcc, spectrogram):\n",
    "        \"\"\"\n",
    "        MFCCからスペクトルを求める.\n",
    "        \"\"\"\n",
    "        im_sp = np.array([])\n",
    "        for i in range(mfcc.shape[0]):\n",
    "            mfcc_s = np.hstack([mfcc[i], [0] * (self.channels - self.dimension)])\n",
    "            mspectrum = scipy.fftpack.idct(mfcc_s, norm='ortho')\n",
    "            # splrep はスプライン補間のための補間関数を求める\n",
    "            tck = scipy.interpolate.splrep(self.fcenters, np.power(10, mspectrum))\n",
    "            # splev は指定座標での補間値を求める\n",
    "            im_spectrogram = scipy.interpolate.splev(self.fscale, tck)\n",
    "            im_sp = np.concatenate((im_sp, im_spectrogram), axis=0)\n",
    "            \n",
    "        return im_sp.reshape(spectrogram.shape)\n",
    "            \n",
    "    def trim_zeros_frames(x, eps=1e-7):\n",
    "        \"\"\"\n",
    "        無音区間を取り除く.\n",
    "        \"\"\"\n",
    "        T, D = x.shape\n",
    "        s = np.sum(np.abs(x), axis=1)\n",
    "        s[s < 1e-7] = 0.\n",
    "        return x[s > eps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_by_world_with_harverst(x, fs):\n",
    "    \"\"\"\n",
    "    WORLD音声分析合成器で基本周波数F0,スペクトル包絡,非周期成分を求める.\n",
    "    基本周波数F0についてはharvest法により,より精度良く求める.\n",
    "    \"\"\"\n",
    "    # 4 Harvest with F0 refinement (using Stonemask)\n",
    "    frame_period = 5\n",
    "    _f0_h, t_h = pw.harvest(x, fs, frame_period=frame_period)\n",
    "    f0_h = pw.stonemask(x, _f0_h, t_h, fs)\n",
    "    sp_h = pw.cheaptrick(x, f0_h, t_h, fs)\n",
    "    ap_h = pw.d4c(x, f0_h, t_h, fs)\n",
    "    \n",
    "    return f0_h, sp_h, ap_h\n",
    "\n",
    "def wavread(file):\n",
    "    \"\"\"\n",
    "    wavファイルから音声トラックとサンプリング周波数を抽出する.\n",
    "    \"\"\"\n",
    "    wf = wave.open(file, \"r\")\n",
    "    fs = wf.getframerate()\n",
    "    x = wf.readframes(wf.getnframes())\n",
    "    x = np.frombuffer(x, dtype= \"int16\") / 32768.0\n",
    "    wf.close()\n",
    "    return x, float(fs)\n",
    "\n",
    "def preEmphasis(signal, p=0.97):\n",
    "    \"\"\"\n",
    "    MFCC抽出のための高域強調フィルタ.\n",
    "    波形を通すことで,高域成分が強調される.\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1.0, -p], 1, signal)\n",
    "\n",
    "def alignment(source, target, path):\n",
    "    \"\"\"\n",
    "    タイムアライメントを取る.\n",
    "    target音声をsource音声の長さに合うように調整する.\n",
    "    \"\"\"\n",
    "    # ここでは814に合わせよう(targetに合わせる)\n",
    "    # p_p = 0 if source.shape[0] > target.shape[0] else 1\n",
    "\n",
    "    #shapes = source.shape if source.shape[0] > target.shape[0] else target.shape \n",
    "    shapes = source.shape\n",
    "    align = np.array([])\n",
    "    for (i, p) in enumerate(path[0]):\n",
    "        if i != 0:\n",
    "            if j != p:\n",
    "                temp = np.array(target[path[1][i]])\n",
    "                align = np.concatenate((align, temp), axis=0)\n",
    "        else:\n",
    "            temp = np.array(target[path[1][i]])\n",
    "            align = np.concatenate((align, temp), axis=0)   \n",
    "        \n",
    "        j = p\n",
    "        \n",
    "    return align.reshape(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "covarXX = np.load(save_for_evgmm_covarXX)\n",
    "covarYX = np.load(save_for_evgmm_covarYX)\n",
    "fitted_source = np.load(save_for_evgmm_fitted_source)\n",
    "fitted_target = np.load(save_for_evgmm_fitted_target)\n",
    "weights = np.load(save_for_evgmm_weights)\n",
    "source_means = np.load(save_for_evgmm_source_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A11.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A14.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A17.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A18.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A19.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A20.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A21.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A22.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A23.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A24.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A25.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A26.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A27.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A28.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A29.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A30.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A31.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A32.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A33.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A34.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A35.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A36.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A37.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A38.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A39.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A40.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A41.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A42.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A43.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A44.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A45.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A46.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A47.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A48.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A49.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A50.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A51.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A52.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A53.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A54.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A55.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A56.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A57.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A58.wav\n",
      "source =  ./utterance/pre-stored0.1.3/input/EJM10/V01/T01/TIMIT/000/A59.wav\n",
      "target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/ATR503/A/A01.wav\n",
      "target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/ATR503/A/A02.wav\n",
      "target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/ATR503/A/A03.wav\n",
      "target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/ATR503/A/A04.wav\n",
      "Load Input and Target Voice time =  65.57809615135193 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "声質変換に用いる変換元音声と目標音声を読み込む.\n",
    "\"\"\"\n",
    "\n",
    "timer_start = time.time()\n",
    "source_mfcc_for_convert = []\n",
    "source_sp_for_convert = []\n",
    "source_f0_for_convert = []\n",
    "source_ap_for_convert = []\n",
    "fs_source = None\n",
    "for name in sorted(glob.iglob(for_convert_source, recursive=True)):\n",
    "    print(\"source = \", name)\n",
    "    x_source, fs_source = sf.read(name)\n",
    "    f0_source, sp_source, ap_source = analyse_by_world_with_harverst(x_source, fs_source)\n",
    "    mfcc_source = MFCC(fs_source)\n",
    "    #mfcc_s_tmp = mfcc_s.mfcc(sp)\n",
    "    #source_mfcc_for_convert = np.hstack([mfcc_s_tmp, mfcc_s.delta(mfcc_s_tmp)])\n",
    "    source_mfcc_for_convert.append(mfcc_source.mfcc(sp_source))\n",
    "    source_sp_for_convert.append(sp_source)\n",
    "    source_f0_for_convert.append(f0_source)\n",
    "    source_ap_for_convert.append(ap_source)\n",
    "\n",
    "target_mfcc_for_fit = []\n",
    "target_f0_for_fit = []\n",
    "target_ap_for_fit = []\n",
    "for name in sorted(glob.iglob(for_convert_target, recursive=True)):\n",
    "    print(\"target = \", name)\n",
    "    x_target, fs_target = sf.read(name)\n",
    "    f0_target, sp_target, ap_target = analyse_by_world_with_harverst(x_target, fs_target)\n",
    "    mfcc_target = MFCC(fs_target)\n",
    "    #mfcc_target_tmp = mfcc_target.mfcc(sp_target)\n",
    "    #target_mfcc_for_fit = np.hstack([mfcc_t_tmp, mfcc_t.delta(mfcc_t_tmp)])\n",
    "    target_mfcc_for_fit.append(mfcc_target.mfcc(sp_target))\n",
    "    target_f0_for_fit.append(f0_target)\n",
    "    target_ap_for_fit.append(ap_target)\n",
    "\n",
    "# 全部numpy.arrrayにしておく\n",
    "source_data_mfcc = np.array(source_mfcc_for_convert)\n",
    "source_data_sp = np.array(source_sp_for_convert)\n",
    "source_data_f0 = np.array(source_f0_for_convert)\n",
    "source_data_ap = np.array(source_ap_for_convert)\n",
    "\n",
    "target_mfcc = np.array(target_mfcc_for_fit)\n",
    "target_f0 = np.array(target_f0_for_fit)\n",
    "target_ap = np.array(target_ap_for_fit)\n",
    "\n",
    "print(\"Load Input and Target Voice time = \", time.time() - timer_start , \"[sec]\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(source, covarXX, fitted_source, fitted_target, covarYX, weights, source_means):\n",
    "    \"\"\"\n",
    "    声質変換を行う.\n",
    "    \"\"\"\n",
    "    Mixtured = 40\n",
    "    \n",
    "    D = source.shape[0]\n",
    "    E = np.zeros((Mixtured, D))\n",
    "\n",
    "    for m in range(Mixtured):\n",
    "        xx = np.linalg.solve(covarXX[m], source - fitted_source[m])\n",
    "        E[m] = fitted_target[m] + np.dot(covarYX[m], xx)\n",
    "\n",
    "    px = GMM(n_components = Mixtured, covariance_type = 'full')\n",
    "    px.weights_ = weights\n",
    "    px.means_ = source_means\n",
    "    px.covars_ = covarXX\n",
    "\n",
    "    posterior = px.predict_proba(np.atleast_2d(source))\n",
    "    return np.dot(posterior, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_mean(input_f0):\n",
    "    \"\"\"\n",
    "    F0変換のために標準偏差と平均を求める.\n",
    "    \"\"\"\n",
    "    tempF0 = input_f0[ np.where(input_f0 > 0)]\n",
    "    fixed_logF0 = np.log(tempF0)\n",
    "    #logF0 = np.ma.log(input_f0) # 0要素にlogをするとinfになるのでmaskする\n",
    "    #fixed_logF0 = np.ma.fix_invalid(logF0).data # maskを取る\n",
    "    return np.std(fixed_logF0), np.mean(fixed_logF0) # 標準偏差と平均を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A11.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A14.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A17.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A18.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A19.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A20.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A21.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A22.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A23.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A24.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A25.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A26.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A27.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A28.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A29.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A30.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A31.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A32.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A33.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A34.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A35.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A36.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A37.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A38.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A39.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A40.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A41.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A42.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A43.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A44.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A45.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A46.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A47.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A48.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A49.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A50.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A51.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A52.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A53.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A54.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A55.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A56.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A57.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A58.wav\n",
      "measure_target =  ./utterance/pre-stored0.1.3/adaptation/EJF01/V01/T01/TIMIT/000/A59.wav\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "距離を測るために,正しい目標音声を読み込む\n",
    "\"\"\"\n",
    "source_mfcc_for_measure_target = []\n",
    "source_sp_for_measure_target = []\n",
    "source_f0_for_measure_target = []\n",
    "source_ap_for_measure_target = []\n",
    "for name in sorted(glob.iglob(for_measure_target, recursive=True)):\n",
    "    print(\"measure_target = \", name)\n",
    "    x_measure_target, fs_measure_target = sf.read(name)\n",
    "    f0_measure_target, sp_measure_target, ap_measure_target = analyse_by_world_with_harverst(x_measure_target, fs_measure_target)\n",
    "    mfcc_measure_target = MFCC(fs_measure_target)\n",
    "    #mfcc_s_tmp = mfcc_s.mfcc(sp)\n",
    "    #source_mfcc_for_convert = np.hstack([mfcc_s_tmp, mfcc_s.delta(mfcc_s_tmp)])\n",
    "    source_mfcc_for_measure_target.append(mfcc_measure_target.mfcc(sp_measure_target))\n",
    "    source_sp_for_measure_target.append(sp_measure_target)\n",
    "    source_f0_for_measure_target.append(f0_measure_target)\n",
    "    source_ap_for_measure_target.append(ap_measure_target)\n",
    "    \n",
    "measure_target_data_mfcc = np.array(source_mfcc_for_measure_target)\n",
    "measure_target_data_sp = np.array(source_sp_for_measure_target)\n",
    "measure_target_data_f0 = np.array(source_f0_for_measure_target)\n",
    "measure_target_data_ap = np.array(source_ap_for_measure_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mcd(source, convert, target):\n",
    "    \"\"\"\n",
    "    変換する前の音声と目標音声でDTWを行う.\n",
    "    その後,変換後の音声と目標音声とのMCDを計測する.\n",
    "    \"\"\"\n",
    "    dist, cost, acc, path = dtw(source, target, dist=lambda x, y: norm(x-y, ord=1))\n",
    "    aligned = alignment(source, target, path)\n",
    "    \n",
    "    return 10.0 / np.log(10) * np.sqrt(2 * np.sum(np.square(aligned - convert))), aligned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voice no =  0\n",
      "voice no =  1\n",
      "voice no =  2\n",
      "voice no =  3\n",
      "voice no =  4\n",
      "voice no =  5\n",
      "voice no =  6\n",
      "voice no =  7\n",
      "voice no =  8\n",
      "voice no =  9\n",
      "voice no =  10\n",
      "voice no =  11\n",
      "voice no =  12\n",
      "voice no =  13\n",
      "voice no =  14\n",
      "voice no =  15\n",
      "voice no =  16\n",
      "voice no =  17\n",
      "voice no =  18\n",
      "voice no =  19\n",
      "voice no =  20\n",
      "voice no =  21\n",
      "voice no =  22\n",
      "voice no =  23\n",
      "voice no =  24\n",
      "voice no =  25\n",
      "voice no =  26\n",
      "voice no =  27\n",
      "voice no =  28\n",
      "voice no =  29\n",
      "voice no =  30\n",
      "voice no =  31\n",
      "voice no =  32\n",
      "voice no =  33\n",
      "voice no =  34\n",
      "voice no =  35\n",
      "voice no =  36\n",
      "voice no =  37\n",
      "voice no =  38\n",
      "voice no =  39\n",
      "voice no =  40\n",
      "voice no =  41\n",
      "voice no =  42\n",
      "voice no =  43\n",
      "voice no =  44\n",
      "Make Converted Spectram time =  612.688894033432 [sec]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "変換を行う.\n",
    "\"\"\"\n",
    "\n",
    "timer_start = time.time()\n",
    "\n",
    "# 事前に目標話者の標準偏差と平均を求めておく\n",
    "temp_f = None\n",
    "for x in range(len(target_f0)):\n",
    "    temp = target_f0[x].flatten()\n",
    "    if temp_f is None:\n",
    "        temp_f = temp\n",
    "    else:\n",
    "        temp_f = np.hstack((temp_f, temp)) \n",
    "target_std, target_mean = calc_std_mean(temp_f)\n",
    "\n",
    "# 変換\n",
    "output_mfcc = []\n",
    "filer = open(mcd_text, 'a')\n",
    "for i in range(len(source_data_mfcc)):   \n",
    "    print(\"voice no = \", i)\n",
    "    # convert\n",
    "    source_temp = source_data_mfcc[i]\n",
    "    output_mfcc = np.array([convert(source_temp[frame], covarXX, fitted_source, fitted_target, covarYX, weights, source_means)[0] for frame in range(source_temp.shape[0])])\n",
    "    \n",
    "    # syntehsis\n",
    "    source_sp_temp = source_data_sp[i]\n",
    "    source_f0_temp = source_data_f0[i]\n",
    "    source_ap_temp = source_data_ap[i]\n",
    "    output_imfcc = mfcc_source.imfcc(output_mfcc, source_sp_temp)\n",
    "    y_source = pw.synthesize(source_f0_temp, output_imfcc, source_ap_temp, fs_source, 5)\n",
    "    np.save(converted_voice_npy + \"s{0}.npy\".format(i), output_imfcc)\n",
    "    sf.write(converted_voice_wav + \"s{0}.wav\".format(i), y_source, fs_source)\n",
    "    \n",
    "    # calc MCD\n",
    "    measure_temp = measure_target_data_mfcc[i]\n",
    "    mcd, aligned_measure = calc_mcd(source_temp, output_mfcc, measure_temp)\n",
    "    filer.write(\"MCD No.{0} = {1} , shape = {2}\\n\".format(i, mcd, source_temp.shape))\n",
    "    \n",
    "    # save figure spectram\n",
    "    range_s = output_imfcc.shape[0]\n",
    "    scale = [x for x in range(range_s)]\n",
    "    MFCC_sample_s = [source_temp[x][0] for x in range(range_s)]\n",
    "    MFCC_sample_c = [output_mfcc[x][0] for x in range(range_s)]\n",
    "    MFCC_sample_t = [aligned_measure[x][0] for x in range(range_s)]\n",
    "    \n",
    "    plt.subplot(311)\n",
    "    plt.plot(scale, MFCC_sample_s, label=\"source\", linewidth = 1.0)\n",
    "    plt.plot(scale, MFCC_sample_c, label=\"convert\", linewidth = 1.0)\n",
    "    plt.plot(scale, MFCC_sample_t, label=\"target\", linewidth = 1.0, linestyle=\"dashed\")\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=3, mode=\"expand\", borderaxespad=0.)\n",
    "    #plt.xlabel(\"Flame\")\n",
    "    #plt.ylabel(\"amplitude MFCC\")\n",
    "    \n",
    "    MFCC_sample_s = [source_temp[x][1] for x in range(range_s)]\n",
    "    MFCC_sample_c = [output_mfcc[x][1] for x in range(range_s)]\n",
    "    MFCC_sample_t = [aligned_measure[x][1] for x in range(range_s)]\n",
    "    \n",
    "    plt.subplot(312)\n",
    "    plt.plot(scale, MFCC_sample_s, label=\"source\", linewidth = 1.0)\n",
    "    plt.plot(scale, MFCC_sample_c, label=\"convert\", linewidth = 1.0)\n",
    "    plt.plot(scale, MFCC_sample_t, label=\"target\", linewidth = 1.0, linestyle=\"dashed\")\n",
    "    plt.ylabel(\"amplitude MFCC\")\n",
    "    \n",
    "    MFCC_sample_s = [source_temp[x][2] for x in range(range_s)]\n",
    "    MFCC_sample_c = [output_mfcc[x][2] for x in range(range_s)]\n",
    "    MFCC_sample_t = [aligned_measure[x][2] for x in range(range_s)]\n",
    "    \n",
    "    plt.subplot(313)\n",
    "    plt.plot(scale, MFCC_sample_s, label=\"source\", linewidth = 1.0)\n",
    "    plt.plot(scale, MFCC_sample_c, label=\"convert\", linewidth = 1.0)\n",
    "    plt.plot(scale, MFCC_sample_t, label=\"target\", linewidth = 1.0, linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Flame\")\n",
    "\n",
    "    plt.savefig(mfcc_save_fig_png + \"s{0}.png\".format(i) , format='png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # synthesis with conveted f0\n",
    "    source_std, source_mean = calc_std_mean(source_f0_temp)\n",
    "    std_ratio = target_std / source_std\n",
    "    log_conv_f0 = std_ratio * (source_f0_temp - source_mean) + target_mean\n",
    "    conv_f0 = np.maximum(log_conv_f0, 0)\n",
    "    np.save(converted_voice_npy + \"f{0}.npy\".format(i), conv_f0)\n",
    "    \n",
    "    y_conv = pw.synthesize(conv_f0, output_imfcc, source_ap_temp, fs_source, 5)\n",
    "    sf.write(converted_voice_with_f0_wav + \"sf{0}.wav\".format(i) , y_conv, fs_source)\n",
    "    \n",
    "    # save figure f0\n",
    "    F0_s = [source_f0_temp[x] for x in range(range_s)]\n",
    "    F0_c = [conv_f0[x] for x in range(range_s)]\n",
    "    \n",
    "    plt.plot(scale, F0_s, label=\"source\", linewidth = 1.0)\n",
    "    plt.plot(scale, F0_c, label=\"convert\", linewidth = 1.0)\n",
    "    plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "    plt.xlabel(\"Frame\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    \n",
    "    plt.savefig(f0_save_fig_png + \"f{0}.png\".format(i), format='png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "filer.close()\n",
    "print(\"Make Converted Spectram time = \", time.time() - timer_start , \"[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
